# -*- coding: utf-8 -*-
"""Specific_taskIV.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1T5Z9F452gQd6GIUjhxH9Q5pdzzjIUQ_W
"""

from google.colab import drive
drive.mount('/content/drive')

!unzip "/content/drive/My Drive/Diffusion/Samples.zip" -d "/content/"

import numpy as np
import matplotlib.pyplot as plt
import glob
import random

# Get all .npy file paths
npy_files = glob.glob("/content/Samples/Samples/*.npy")

# Select 10 random files
num_images = min(10, len(npy_files))  # Ensure we don't exceed available images
random_files = random.sample(npy_files, num_images)

# Create a grid of 2 rows, 5 columns
fig, axes = plt.subplots(2, 5, figsize=(15, 6))

for i, ax in enumerate(axes.flat):
    img_array = np.load(random_files[i]).squeeze(0)  # Remove singleton dimension
    ax.imshow(img_array, cmap='gray')
    ax.set_title(f"Image {i+1}")
    ax.axis("off")  # Hide axis labels

plt.tight_layout()
plt.show()

# Fid = 299(avg) and lowest at 213 at 20th epoch
# ======================
# 1. Setup & Imports
# ======================
!pip install torchvision pytorch-fid
import os
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader, random_split
from torchvision.utils import save_image
from torchvision.models import inception_v3
from torchvision.transforms import Resize
import numpy as np
import os
from tqdm import tqdm
from IPython.display import Image, display, clear_output
from scipy.linalg import sqrtm
import math
from torchvision import models
import torch.optim as optim
from torch.optim.lr_scheduler import CosineAnnealingLR
# ======================
# 2. Configuration
# ======================
device = 'cuda' if torch.cuda.is_available() else 'cpu'
print(f"Using device: {device}")

# Model parameters (original values)
image_size = 150
batch_size = 32
T = 1000
n_epochs = 40  # Original epoch count
eval_interval = 5
fid_sample_size = 500

# Data parameters
data_dir = '/content/Samples/Samples/'
train_ratio = 0.9

# Create output directories
os.makedirs('/content/samples', exist_ok=True)
os.makedirs('/content/checkpoints', exist_ok=True)

# ======================
# 3. Dataset
# ======================
class LensDataset(Dataset):
    def __init__(self, root_dir):
        self.files = [os.path.join(root_dir, f) for f in os.listdir(root_dir) if f.endswith('.npy')]

    def __len__(self):
        return len(self.files)

    def __getitem__(self, idx):
        img = np.load(self.files[idx])
        img = torch.tensor(img).float()
        return (img / 127.5) - 1.0

full_dataset = LensDataset(data_dir)
train_size = int(len(full_dataset) * train_ratio)
test_size = len(full_dataset) - train_size
train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])

train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

print(f"Training samples: {len(train_dataset)}, Test samples: {len(test_dataset)}")
#loss
# ======================# tried to make a custom loss function to keep the symmetry of archs but was not working well since weights are a problem and no reaseach literature regarding weights

# def fourier_loss(pred, target):
#     pred_fft = torch.fft.fft2(pred)
#     target_fft = torch.fft.fft2(target)
#     return F.mse_loss(torch.abs(pred_fft), torch.abs(target_fft))

# def ssim_loss(pred, target):
#     C1, C2 = 0.01 ** 2, 0.03 ** 2
#     mu_x, mu_y = pred.mean([2, 3]), target.mean([2, 3])
#     sigma_x, sigma_y = pred.var([2, 3]), target.var([2, 3])
#     sigma_xy = ((pred - mu_x[:, :, None, None]) * (target - mu_y[:, :, None, None])).mean([2, 3])
#     ssim = (2 * mu_x * mu_y + C1) * (2 * sigma_xy + C2) / ((mu_x ** 2 + mu_y ** 2 + C1) * (sigma_x + sigma_y + C2))
#     return 1 - ssim.mean()

# class PerceptualLoss(nn.Module):
#     def __init__(self):
#         super().__init__()
#         self.resize = Resize((224, 224))
#         vgg = models.vgg16(weights=models.VGG16_Weights.DEFAULT).features[:16]
#         for param in vgg.parameters():
#             param.requires_grad = False
#         self.vgg = vgg.eval()

#     def forward(self, x, y):
#         # Convert grayscale to RGB by repeating the channel
#         if x.shape[1] == 1:
#             x = x.repeat(1, 3, 1, 1)
#         if y.shape[1] == 1:
#             y = y.repeat(1, 3, 1, 1)
#         x = self.resize(x)
#         y = self.resize(y)
#         return F.mse_loss(self.vgg(x), self.vgg(y))

# Initialize losses
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# perceptual_loss = PerceptualLoss().to(device)
mse_loss = nn.MSELoss()


# 4. UNet Model
# ======================

import torch
import torch.nn as nn
import torch.nn.functional as F
import math

class UNet(nn.Module): #wanted to add self attention but collab memory limits also minimized the channel no due to memory limits
    def __init__(self, c_in=1, c_out=1, time_dim=128):
        super().__init__()
        self.time_dim = time_dim

        # Time embedding
        self.time_embed = nn.Sequential(
            nn.Linear(time_dim, time_dim),
            nn.ReLU(),
            nn.Linear(time_dim, time_dim),
            nn.ReLU()
        )

        # Downsampling
        self.down1 = self._block(c_in, 32)
        self.down2 = self._block(32, 64, downsample=True)
        self.down3 = self._block(64, 128, downsample=True)

        # Bottleneck
        self.mid = self._block(128, 128)

        # Upsampling
        self.up1 = self._block(128 + 128, 64)
        self.up2 = self._block(64 + 64, 32)
        self.up3 = self._block(32 + 32, 32)

        # Output layer
        self.out = nn.Conv2d(32, c_out, 1)

    def _block(self, in_c, out_c, downsample=False):
        layers = [
            nn.Conv2d(in_c, out_c, 3, padding=1, stride=(2 if downsample else 1)),
            nn.GroupNorm(4, out_c),  # Fewer groups to reduce memory
            nn.ReLU(),
            nn.Conv2d(out_c, out_c, 3, padding=1),
            nn.GroupNorm(4, out_c),
            nn.ReLU()
        ]
        return nn.Sequential(*layers)

    def time_embedding(self, t):
        emb = torch.sin(t[:, None] * torch.arange(self.time_dim, device=t.device)[None, :])
        return self.time_embed(emb).view(-1, self.time_dim, 1, 1)

    def forward(self, x, t):
        t_emb = self.time_embedding(t)

        # Downsampling
        d1 = self.down1(x)
        d2 = self.down2(d1)
        d3 = self.down3(d2)

        # Bottleneck
        m = self.mid(d3) + t_emb

        # Upsampling
        u1 = self.up1(torch.cat([F.interpolate(m, size=d3.shape[2:]), d3], dim=1))
        u2 = self.up2(torch.cat([F.interpolate(u1, size=d2.shape[2:]), d2], dim=1))
        u3 = self.up3(torch.cat([F.interpolate(u2, size=d1.shape[2:]), d1], dim=1))

        return self.out(u3)

# Move to device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = UNet().to(device)


# ======================
# 5. Diffusion Setup
# ======================
def cosine_beta_schedule(timesteps, s=0.008):
    steps = timesteps + 1
    x = torch.linspace(0, timesteps, steps)
    alphas_cumprod = torch.cos(((x / timesteps) + s) / (1 + s) * math.pi * 0.5) ** 2
    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]
    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])
    return torch.clip(betas, 0.0001, 0.02)

beta = cosine_beta_schedule(T).to(device)
alpha = 1 - beta
alpha_hat = torch.cumprod(alpha, dim=0)

def noise_image(x, t):
    noise = torch.randn_like(x)
    sqrt_alpha_hat = torch.sqrt(alpha_hat[t]).view(-1, 1, 1, 1)
    sqrt_one_minus_alpha_hat = torch.sqrt(1 - alpha_hat[t]).view(-1, 1, 1, 1)
    return sqrt_alpha_hat * x + sqrt_one_minus_alpha_hat * noise, noise

def generate_samples(num_samples=16):
    model.eval()
    with torch.no_grad():
        x = torch.randn(num_samples, 1, image_size, image_size, device=device)
        for t in reversed(range(T)):
            t_tensor = torch.full((num_samples,), t, device=device, dtype=torch.long)
            noise_pred = model(x, t_tensor)
            if t > 0:
                beta_t = beta[t].view(-1, 1, 1, 1)
                alpha_t = alpha[t].view(-1, 1, 1, 1)
                alpha_hat_t = alpha_hat[t].view(-1, 1, 1, 1)
                z = torch.randn_like(x) if t > 0 else 0
                x = (1 / torch.sqrt(alpha_t)) * (x - beta_t / torch.sqrt(1 - alpha_hat_t) * noise_pred) + torch.sqrt(beta_t) * z

    return x.cpu()
# 6. Fixed FID Evaluation Setup
# ======================
# Initialize Inception model with aux_logits=True

inception = inception_v3(pretrained=True, transform_input=False, aux_logits=True).eval().to(device)
inception.fc = nn.Identity()  # Remove final classification layer

# ======================
# 6. FID Evaluation Setup
# ======================
def get_activations(images, model, batch_size=32):
    activations = []
    resize = Resize((299, 299))  # Resize to match InceptionV3 input size
    mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1).to(device)  # Normalize for Inception
    std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1).to(device)

    with torch.no_grad():
        for i in range(0, len(images), batch_size):
            batch = images[i:i+batch_size].to(device)
            batch = batch.repeat(1, 3, 1, 1)  # Convert grayscale to RGB by repeating channels
            batch = resize(batch)  # Resize to 299x299 for Inception
            batch = (batch + 1) / 2  # Scale from [-1, 1] to [0, 1]
            batch = (batch - mean) / std  # Normalize to [0, 1] range

            # Get activations from the Inception model
            act = model(batch)
            if isinstance(act, tuple):  # Handle the case where aux_logits=True
                act = act[0]
            activations.append(act.cpu())
    return torch.cat(activations).numpy()

def calculate_fid(real_acts, fake_acts, eps=1e-6):
    mu1, sigma1 = real_acts.mean(axis=0), np.cov(real_acts, rowvar=False)
    mu2, sigma2 = fake_acts.mean(axis=0), np.cov(fake_acts, rowvar=False)

    sigma1 += eps * np.eye(sigma1.shape[0])
    sigma2 += eps * np.eye(sigma2.shape[0])

    covmean = sqrtm(sigma1.dot(sigma2))
    if np.iscomplexobj(covmean):
        covmean = covmean.real

    return np.sum((mu1 - mu2)**2) + np.trace(sigma1 + sigma2 - 2*covmean)

# Initialize Inception model (works with both aux_logits=True/False)
try:
    inception = inception_v3(pretrained=True, transform_input=False, aux_logits=False).eval().to(device)
except ValueError:
    inception = inception_v3(pretrained=True, transform_input=False, aux_logits=True).eval().to(device)
inception.fc = nn.Identity()

# ======================
# 7. Training Loop with LR Display
# ======================
optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4)

# # Hybrid scheduler setup # tried hybrid scheduler(gradual and onplateaue) but it was not working effectively
# warmup_epochs = int(0.1 * n_epochs)
# plateau_start = int(0.7 * n_epochs)

# # Warmup scheduler
# warmup_scheduler = torch.optim.lr_scheduler.LambdaLR(
#     optimizer,
#     lr_lambda=lambda e: min(1.0, e / warmup_epochs)
# )

# # Plateau scheduler
# plateau_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
#     optimizer,
#     mode='min',
#     factor=0.5,
#     patience=2,
#     verbose=True
# )
scheduler = CosineAnnealingLR(optimizer, T_max=40, eta_min=1e-6)

for epoch in range(1, n_epochs + 1):
    model.train()
    epoch_loss = 0

    # Get current learning rate
    current_lr = optimizer.param_groups[0]['lr']

    for batch in tqdm(train_loader,
                    desc=f"Epoch {epoch}/{n_epochs} (LR: {current_lr:.1e})",
                    leave=True):
        x = batch.to(device)
        t = torch.randint(0, T, (x.shape[0],), device=device)

        x_noisy, noise = noise_image(x, t)
        pred_noise = model(x_noisy, t)
        loss_mse = mse_loss(pred_noise, noise)
        # loss_perceptual = perceptual_loss(pred_noise, noise)
        # loss_fourier = fourier_loss(pred_noise, noise)
        # loss_ssim = ssim_loss(pred_noise, noise)

        # loss = loss_mse * 1.0 + loss_perceptual * 0.05 + loss_fourier * 0.5 + loss_ssim * 0.3
        loss=loss_mse
        optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Gradient clipping
        optimizer.step()

        epoch_loss += loss.item()

    # # Hybrid scheduler step
    # if epoch < warmup_epochs:
    #     warmup_scheduler.step()
    # elif epoch >= plateau_start:
    #     plateau_scheduler.step(epoch_loss)
    scheduler.step()

    avg_loss = epoch_loss / len(train_loader)
    # phase = 'warmup' if epoch < warmup_epochs else 'plateau' if epoch >= plateau_start else 'constant'

    # Enhanced epoch print with all details
    print(f"\nEpoch {epoch:03d}/{n_epochs} | "
          f"Loss: {avg_loss:.4f} | "
          f"LR: {current_lr:.3e} | "
          f"GPU Mem: {torch.cuda.memory_allocated()/1e6:.1f}MB")

    if epoch % eval_interval == 0 or epoch == n_epochs:
        samples = generate_samples(16)
        sample_path = f"/content/samples/epoch_{epoch}.png"
        save_image((samples + 1)/2, sample_path, nrow=4)
        display(Image(sample_path))

        # FID Calculation
        if len(test_dataset) >= fid_sample_size:
            try:
                real_samples = torch.stack([test_dataset[i] for i in range(fid_sample_size)])
                fake_samples = generate_samples(fid_sample_size)
                torch.cuda.empty_cache()
                torch.cuda.ipc_collect()

                real_acts = get_activations(real_samples, inception)
                fake_acts = get_activations(fake_samples, inception)

                fid_score = calculate_fid(real_acts, fake_acts)
                print(f"FID Score: {fid_score:.2f}")

            except Exception as e:
                print(f"FID calculation error: {str(e)}")
        else:
            print(f"Skipping FID - Need {fid_sample_size} samples, have {len(test_dataset)}")

        torch.save({
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'loss': avg_loss,
            'lr': current_lr
        }, f'/content/checkpoints/epoch_{epoch}.pt')

print("Training complete!")

# ======================
# 8. Final Evaluation
# ======================
print("\nFinal samples:")
for fname in sorted(os.listdir('/content/samples')):
    if fname.endswith('.png'):
        display(Image(f"/content/samples/{fname}"))

if len(test_dataset) >= fid_sample_size:
    try:
        final_real = torch.stack([test_dataset[i] for i in range(fid_sample_size)])
        final_fake = generate_samples(fid_sample_size)
        final_fid = calculate_fid(
            get_activations(final_real, inception),
            get_activations(final_fake, inception))
        print(f"\nFinal FID: {final_fid:.2f}")
    except Exception as e:
        print(f"Final FID failed: {str(e)}")

!zip -r /content/results.zip /content/samples /content/checkpoints



